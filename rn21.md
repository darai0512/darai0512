<!-- .slide: data-background="img/kotoba-logo-bg.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.5"-->

## RN (等)でRealtime
## Voice AIプロダクトを作る

darai: Kotoba Technologies

@React Native Meetup #21

<!-- title/アプリ紹介 -->
<a href="https://apps.apple.com/app/id6740851285" target="_blank"><img src="./img/dotsu.png" width="80" height="80"/></a>

---

<!-- .slide: data-background="img/kotoba-logo-bg.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.5"-->

<iframe src="./index.html" style="position:fixed; top:0; left:0; bottom:0; right:0; width:100%; height:100%; border:none; margin:0; padding:0; overflow:hidden; z-index:999999; min-height: 660px;">

---

<!-- .slide: data-background="img/kotoba-logo-bg.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.1"-->

## bare ReactNative 採用理由

---

<!-- .slide: data-background="img/kotoba-logo-bg.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.1"-->

1. Next.js でWebのDemo 
1. (bare) ReactNative (v76) でiOS & Web Product
  - Web はviteに
  - Audio Streamingライブラリが
1. tauri でMac & Windows Product
  - ReactNative Windows & Mac は? :thinking_face:

<hr/>

- プロダクト化はマルチプラットフォームで！
- 少人数(1,2人)なのでコードを共通化したい
- Audio Streamingをnative実装で todo
  - safari nextでの難しさ

Note: Kotobaテクノロジーズでは、iOSアプリ/デスクトップアプリ/ウェブアプリで音声AIプロダクトを開発しています。
専属エンジニアは自分1人です。なのでワンコードですべてのプラットフォームで使えるソリューションを探していました。

---

<!-- .slide: data-background="img/system_audio.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-position="right"-->
<!-- .slide: data-background-opacity="0.3"-->

### Tauri (v2) も使い始めた

- 要件: システムオーディオ取りたい
  - iOS AUE/Android AudioPlaybackCaptureはコラボ前提
  - BlackHole/VB-Cable 無しで音声キャプチャしたい
  - OSX: [Audio Tap API](https://developer.apple.com/documentation/coreaudio/capturing-system-audio-with-core-audio-taps)
    - Web Audio API/MediaDevices APIのデバイスとは別
    - [Electronではサポート外](https://www.electronjs.org/ja/docs/latest/api/desktop-capturer#caveats)
- どのエコシステムでも1コードじゃ済まない
  - 拡張子でimport moduleが変わるのは素晴らしいけれど

エコシステム比較 | fork | star | latest commit
--- | --- | --- | ---
[Tauri](https://github.com/tauri-apps/tauri) | 2.8k | 91.4k | 5h
[RN win](https://github.com/microsoft/react-native-windows) | 1.2k | 16.7k | 2days
[RN mac](https://github.com/microsoft/react-native-macos) | 146 | 3.8k | 3days

Note: 

Audio Unit Extensions: AUE 旧Inter-App Audio: IAA, audiobusなどDTM向け.他のアプリのオーディオ出力をキャプチャしてミキシングすることが可能

Picture in Picture+Dotsuで音声を取れないか？=>試したら片方が強制的にオフになり不可

Android manifest: allowAudioPlaybackCapture
AudioPlaybackCapture APIを使えば可能だが、キャプチャ元のアプリの明示的な許可設定が必要
ほとんどのアプリが不可な様子。chromium系は一括でfalseぽい
https://source.chromium.org/chromium/chromium/src/+/main:chrome/android/java/AndroidManifest.xml;l=214;bpv=1;bpt=0;drc=cc2a44eb4b27f696db7cd142be203d5aca9f2bdc

tauri
backendがrustでC系と結合しやすいため、win/macのネイティブAPIを触るには良い
Electronではシステム音声キャプチャがサポート外であることはドキュメントに書かれている

ネイティブAPIを触る必要があるなら、ネイティブ実装の方がシンプルで早いかと
- Tauriなら、メモリ、所有権、スレッド間共有あたりを丁寧にやらないと死ぬことがわかった（あとデバッグがむずい）

------

<!-- .slide: data-background="img/system_audio.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-position="right"-->

---

<!-- .slide: data-background="img/kotoba-logo-bg.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.1"-->

### Appendix. Expo機能をbare RNで使う

- OTAアップデート
  - [MicroSoft code push](https://github.com/microsoft/code-push-server): サービス終了、OSS化
  - [react-native-ota-hot-update](https://github.com/vantuan88291/react-native-ota-hot-update): 40 fork/419 star
  - [hot-updater](https://github.com/gronxb/hot-updater): 41 fork/541 star
    - どちらもNew architecture対応
- Clerk
  - [`@clerk/clerk-expo`](https://clerk.com/docs/references/expo/overview): 公式SDK
  - bare RNから使うにはProviderを作って `@clerk/clerk-react` を
  
Note: 野良実装を参考. star 0だけど実装は参考になる

---

<!-- .slide: data-background="img/kotoba-logo-bg.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.1"-->

## Realtime通信とVoice Data

---

<!-- .slide: data-background="img/opus15.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.1"-->

### Web/電話回線で Realtime通信するなら

API | 遅延 | Protocol | 説明
--- | ---| ---
WebRTC | ~0.1s | RTP他 | 準備大変,今のClient最有力,[opus1.5](https://speakerdeck.com/tetter27/webrtc-to-wasm-noguan-xi-wozhen-rifan-tutemita?slide=16)/[MLow](https://zenn.dev/yuki_uchida/articles/d4f32375cde297)期待
WebSocket | ~0.1s | HTTP1 | 簡単、Client/Server/電話、mqttも話せる
gRPC | ? | HTTP2 | Webの実装待ち?
RTMP | 5s | TCP |
HLS | 10s | HTTP1/2 |
HTTP chunked encoding | ? | HTTP1 | OpenAI Streaming APIなどで使われる

Note: 遅延はaudio
meta社員もRTCおすすめ
mqtt over websocket
https://qiita.com/emqx_japan/items/44494fe8ed29eb0c2521

RTMP遅延 Real-Time Messaging Protocol
https://www.dpsj.co.jp/tech-articles/wowza-blog-rtmp

HLS遅延 HTTP Live Streaming
https://note.com/standfm_company/n/nc9d5eb129bc7

chromeのopusのversion : v1.3.1 +a
https://chromium.googlesource.com/chromium/src/+/HEAD/third_party/opus/README.chromium のcommit hash
https://opus-codec.org/development/ からの
https://gitlab.xiph.org/xiph/opus/-/commit/8cf872a186b96085b1bb3a547afd598354ebeb87 2022/11
opus 1.5 release 2024年3月4日

------

<!-- .slide: data-background="img/opus15.png""-->
<!-- .slide: data-background-size="contain"-->

出典: WebRTC と Wasm の関係を振り返ってみた by tetter
https://speakerdeck.com/tetter27/webrtc-to-wasm-noguan-xi-wozhen-rifan-tutemita?slide=16

---

<!-- .slide: data-background="img/kotoba-logo-bg.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.1"-->

### Voice AI界隈のRealtime API

- OpenAI Realtime API
  - WebSocket or WebRTC
  - input format `pcm16`, `g711_ulaw` or `g711_alaw` 24kHz (※)
- baidu
  - WebSocket
  - input format `pcm16` 16kHz
- Kotoba Tech
  - 近日WebSocket版をリリース予定

※ 16-bit PCM at a 24kHz sample rate, 1 channel, and little-endian byte order

Note:
https://aca.bce.baidu.com/doc/SPEECH/s/jlbxejt2i
https://platform.openai.com/docs/guides/realtime

G.711 A-law (g711_alaw)
	•	使用地域:
主にヨーロッパや多くの国際標準（ITU-Tで採用）で利用されます。
	•	特徴:
	•	A-law は、動的レンジ（音の強弱の幅）を広げつつ、雑音の影響を軽減するためのコンパウンディング（圧縮）アルゴリズムです。

  G.711 μ-law (g711_ulaw)
	•	使用地域:
主に北米や日本などで採用されています。
	•	特徴:
	•	μ-law は、A-law に比べてより高い圧縮率を実現し、低信号レベルでの分解能をさらに向上させる設計になっています。

主な違いと使い分け
	•	コンパウンディングの違い:
A-law と μ-law は、非線形にリニアPCM信号を圧縮するための異なる数式や圧縮パラメーター（圧縮曲線）を持っています。
	•	A-law:
線形部分がより大きいように設計され、低レベル信号の解像度を抑える一方で、圧縮時の雑音が低減される傾向があります。
	•	μ-law:
低レベル信号に対してより高い分解能が確保されるため、特に北米や日本において有利とされています。
	•	地域の慣習:
地域標準や既存の通信ネットワークに合わせ、ヨーロッパでは A-law、北米・日本では μ-law が採用されることが多いです。
	•	通信ネットワーク間での互換性のために、どちらか一方のアルゴリズムが採用されている場合があり、エンドツーエンドで同じアルゴリズムを使用する必要があります。
	•	音質やダイナミックレンジ:
両者とも 64 kbps という同等のビットレートですが、圧縮曲線の違いにより、同じ環境下でもわずかに音質やダイナミックレンジの感じ方が異なります。用途や環境に応じて最適な方が選ばれます。

---

<!-- .slide: data-background="img/audio.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.1"-->

### PCM? SamplingRate? 

- PCM（パルス符号変調）: 連続値である音を時間軸上の離散的な振幅値として記録する方法
- 振幅: 16bit = 2byte = -32768~32768 で音の強度を表す
- SamplingRate: ある時刻における振幅の値
  -  Rateが高い=標本点が多い=再現度

Format
- pcm16
- audio/wav: pcmにヘッダをつけただけ
- audio/ogg;codec=opus: ogg形式かつopusで圧縮

Note:
音の他の要素は? v=fr=高低, 振幅=大きさ
pcm16, g711_ulaw, or g711_alaw
input audio must be 16-bit PCM at a 24kHz sample rate, single channel (mono), and little-endian byte order.

------

<!-- .slide: data-background="img/audio.png""-->
<!-- .slide: data-background-size="contain"-->

出典: データサイエンスと音声処理技術　①音声データ処理 (S2-109) https://www.youtube.com/watch?v=mMeAwAsnpqc

---

### Chunkサイズ計算式

<!-- .slide: data-background="img/kotoba-logo-bg.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.1"-->

```js
import {...} from "@dr.pogodin/react-native-audio";

const stream = new InputAudioStream(
  AUDIO_SOURCES.MIC,
  16000, // Sample rate in Hz.
  CHANNEL_CONFIGS.MONO, // 1 channel cf, chromeは2ch
  AUDIO_FORMATS.PCM_16BIT, // pcm16
  3200, // Sampling size.
  false // stopInBackground
)
stream.addChunkListener((chunk, chunkId) => {
  webSocketRef.current.send(new Int16Array(chunk.buffer));
}) // => 3200/16000=0.2s おきに 3200*2byte*1=6.4kb => 32kbps
stream.start()
```

cf, Safari/iOS WebViewだと48kHz固定、2ch => 6倍多くなる

cf, [Meta調べ](https://zenn.dev/yuki_uchida/articles/d4f32375cde297)だとユーザーの
25%は 250kbps 未満, 50%は 400kbps 未満, 95%は 450kbps 未満

Note:
今僕の方340バイトおきに音声バッファ処理されててあまりにも短い気がしており
30s voice 200msで捌く

RTCでは16KBにまでにすべき RFC8831
https://speakerdeck.com/sublimer/motutoda-kinatetawosong-rimasenka-erakakorokorochu-ruyounatetatesu?slide=9

---

<!-- .slide: data-background="img/kotoba-logo-bg.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.1"-->

### スケールする実装のために

- chunk sizeとネットワーク帯域
- GPU確保が難しくBE Regionが日本とは限らない
- 流量制御
  - [NTT Comm @sublimerさんのWebRTCでの制御の話](https://speakerdeck.com/sublimer/motutoda-kinatetawosong-rimasenka-erakakorokorochu-ruyounatetatesu?slide=12)
  - [Node.js Core API: Stream](https://nodejs.org/docs/latest/api/stream.html) のback-pressure, HighWaterMark
  - HTTP ならleaky bucketアルゴリズム

Note:
python FastAPIだと自前で

---

<!-- .slide: data-background="img/hiring.png""-->
<!-- .slide: data-background-size="contain"-->
<!-- .slide: data-background-opacity="0.3"-->
<!-- .slide: style="text-align: center;"-->

### 終わり

s2s はさらに難度が上がります。

またLTできれば :laughing: 

Note: https://github.com/kyutai-labs/moshi/issues/77

---

<!-- .slide: data-background="img/hiring.png""-->
<!-- .slide: data-background-size="contain"-->

<!--embed src="./kotoba_hiring.pdf" type="application/pdf" style="width: 100vw; height: 100vh;"-->

---

<!-- .slide: data-background="img/study.png""-->
<!-- .slide: data-background-size="contain"-->